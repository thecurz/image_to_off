{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm  \n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Activation\n",
    "from keras.layers import Embedding, Lambda, Concatenate, Add\n",
    "from keras.layers import Conv3DTranspose, Conv3D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "import keras.backend as K\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Activation\n",
    "from keras.layers import Embedding, Lambda, Concatenate, Add\n",
    "from keras.layers import GlobalAvgPool3D, Multiply, GlobalAveragePooling3D\n",
    "from keras.layers import Conv3DTranspose, Conv3D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import LeakyReLU\n",
    "import keras.backend as K\n",
    "from vedo import load, write, Points\n",
    "\n",
    "\n",
    "dataset_path = \"ModelNet40\"\n",
    "images_path = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_layer(inp, f, act='relu', bn=True):\n",
    "    initializer = act if act is not None else ''\n",
    "    initializer = 'he_uniform' if initializer.find('relu') != -1 else 'glorot_uniform'\n",
    "    out = Dense(f, use_bias=False, kernel_initializer=initializer)(inp)\n",
    "    if bn: out = BatchNormalization()(out)\n",
    "    \n",
    "    if act == 'lrelu':\n",
    "        out = LeakyReLU(alpha=0.2)(out)\n",
    "    elif act is not None:\n",
    "        out = Activation(act)(out)\n",
    "    \n",
    "    return out\n",
    "\n",
    "def conv_layer(inp, f, k=4, s=2, p='same', act='relu', bn=True, transpose=False,\n",
    "               se=False, se_ratio=16):\n",
    "    initializer = act if act is not None else ''\n",
    "    initializer = 'he_uniform' if initializer.find('relu') != -1 else 'glorot_uniform'\n",
    "    fun = Conv3DTranspose if transpose else Conv3D\n",
    "    out = fun(f, k, strides=s, padding=p, use_bias=False, kernel_initializer=initializer)(inp)\n",
    "    if bn: out = BatchNormalization()(out)\n",
    "    \n",
    "    if act == 'lrelu':\n",
    "        out = LeakyReLU(alpha=0.2)(out)\n",
    "    elif act is not None:\n",
    "        out = Activation(act)(out)\n",
    "\n",
    "    # squeeze and excite\n",
    "    if se:\n",
    "        out_se = GlobalAvgPool3D()(out)\n",
    "        r = f // se_ratio if (f // se_ratio) > 0 else 1\n",
    "        out_se = Reshape((1, 1, f))(out_se)\n",
    "        out_se = Dense(r, use_bias=False, kernel_initializer='he_uniform',\n",
    "                       activation='relu')(out_se)\n",
    "        out_se = Dense(f, use_bias=False, activation='sigmoid')(out_se)\n",
    "        out = Multiply()([out, out_se])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _generator_v_3d(dict_size):\n",
    "#     # inputs\n",
    "#     labels = Input(shape=(1,))\n",
    "#     image_inp = Input(shape=(128, 128, 1))\n",
    "#     image = Lambda(lambda x: K.expand_dims(x))(image_inp)\n",
    "    \n",
    "#     # label embedding\n",
    "#     embs = Embedding(dict_size, 64, input_length=1)(labels)\n",
    "#     embs = Flatten()(embs)\n",
    "#     embs = dense_layer(embs, 1024)\n",
    "    \n",
    "#     # conv layers for image processing\n",
    "#     image_conv = Conv2D(32, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")(image)\n",
    "#     image_conv = Conv2D(32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(image_conv)\n",
    "#     image_conv = Conv2D(64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(image_conv)\n",
    "#     # image_conv = Conv2D(128, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(image_conv)\n",
    "#     # image_conv = Conv2D(256, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(image_conv)\n",
    "    \n",
    "#     image_flatten = Flatten()(image_conv)\n",
    "    \n",
    "#     # Merge label embedding and processed image\n",
    "#     mix = Concatenate()([image_flatten, embs])\n",
    "#     mix = dense_layer(mix, 1024)\n",
    "#     mix = dense_layer(mix, 2*2*2*256)\n",
    "#     mix = Lambda(lambda x: K.reshape(x, (-1, 2, 2, 2, 256)))(mix)\n",
    "\n",
    "#     # Transpose convolution layers\n",
    "#     out = Conv3DTranspose(256, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\", activation=\"relu\")(mix)\n",
    "#     out = Conv3DTranspose(128, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\", activation=\"relu\")(out)\n",
    "#     out = Conv3DTranspose(64, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\", activation=\"relu\")(out)\n",
    "#     out = Conv3DTranspose(32, kernel_size=(3, 3, 3), strides=(2, 2, 2), padding=\"same\", activation=\"relu\")(out)\n",
    "    \n",
    "#     # Output layer\n",
    "#     out = Conv3D(1, kernel_size=(3, 3, 3), padding=\"same\", activation=\"tanh\")(out)\n",
    "    \n",
    "#     return Model((image_inp, labels), out)\n",
    "def _generator_v_3d(dict_size):\n",
    "    # Input layers\n",
    "    labels = Input(shape=(1,))\n",
    "    image_inp = Input(shape=(128, 128, 1))\n",
    "    \n",
    "    # Label embedding\n",
    "    embs = Embedding(dict_size, 32, input_length=1)(labels)\n",
    "    embs = Flatten()(embs)\n",
    "    embs = dense_layer(embs, 512)\n",
    "    \n",
    "    # Convolutional layers for image processing\n",
    "    image_conv = Conv2D(16, kernel_size=5, strides=1, padding=\"same\", activation=\"relu\")(Lambda(lambda x: K.expand_dims(x))(image_inp))\n",
    "    image_conv = Conv2D(16, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(image_conv)\n",
    "    image_conv = Conv2D(32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")(image_conv)\n",
    "    \n",
    "    # Flattening and merging label embedding and processed image\n",
    "    mix = Flatten()(image_conv)\n",
    "    mix = dense_layer(mix, 512)\n",
    "    mix = dense_layer(mix, 2 * 2 * 2 * 128)\n",
    "    mix = Lambda(lambda x: K.reshape(x, (-1, 2, 2, 2, 128)))(mix)\n",
    "\n",
    "    # Transpose convolution layers\n",
    "    out = conv_layer(mix, 128, k=(3, 3, 3), s=(2, 2, 2), transpose=True, p=\"same\", act=\"relu\")\n",
    "    out = conv_layer(out, 64, k=(3, 3, 3), s=(2, 2, 2), transpose=True, p=\"same\", act=\"relu\")\n",
    "    out = conv_layer(out, 32, k=(3, 3, 3), s=(2, 2, 2), transpose=True, p=\"same\", act=\"relu\")\n",
    "    out = conv_layer(out, 16, k=(3, 3, 3), s=(2, 2, 2), transpose=True, p=\"same\", act=\"relu\")\n",
    "    \n",
    "    # Output layer\n",
    "    out = Conv3D(1, kernel_size=(3, 3, 3), padding=\"same\", activation=\"tanh\")(out)\n",
    "    \n",
    "    return Model([image_inp, labels], out)\n",
    "# Discriminator models:\n",
    "\n",
    "def _discriminator(dict_size, se=False):\n",
    "    # inputs\n",
    "    labels = Input(shape=(1,))\n",
    "    voxels_inp = Input(shape=(32,32,32))\n",
    "    #voxels_inp = Input(shape=(32, 32, 32, 1))\n",
    "    voxels = Lambda(lambda x: K.expand_dims(x))(voxels_inp)\n",
    "    # label embedding\n",
    "    embs = Embedding(dict_size, 64, input_length=1)(labels)\n",
    "    embs = Flatten()(embs)\n",
    "    embs = dense_layer(embs, 1024, act='lrelu', bn=False)\n",
    "    \n",
    "    # conv layers\n",
    "    out = conv_layer(voxels, 32, 5, 1, act='lrelu', bn=False, se=se)\n",
    "    out = conv_layer(out, 32, act='lrelu', bn=False, se=se)\n",
    "    out = conv_layer(out, 64, act='lrelu', bn=False, se=se)\n",
    "    out = conv_layer(out, 128, act='lrelu', bn=False, se=se)\n",
    "    out = conv_layer(out, 256, act='lrelu', bn=False, se=se)\n",
    "    out = Flatten()(out)\n",
    "    out = dense_layer(out, 1024, act='lrelu', bn=False)\n",
    "    out = Concatenate()([out, embs])\n",
    "    out = dense_layer(out, 1024, act='lrelu', bn=False)\n",
    "    out = dense_layer(out, 512, act='lrelu', bn=False)\n",
    "    out = dense_layer(out, 1, act=None, bn=False)\n",
    "    \n",
    "    return Model((voxels_inp, labels), out)\n",
    "\n",
    "def make_discriminator(dict_size, model_type):\n",
    "    model = {\n",
    "        'voxels-v': _discriminator(dict_size),\n",
    "        'voxels-u': _discriminator(dict_size),\n",
    "        'voxels-use': _discriminator(dict_size, se=True)\n",
    "    }\n",
    "    \n",
    "    return model[model_type]\n",
    "\n",
    "def make_generator(dict_size, model_type):\n",
    "    model = {\n",
    "        'voxels-v': _generator_v_3d(dict_size),\n",
    "    }\n",
    "    \n",
    "    return model[model_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_files(folder_path):\n",
    "    model_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".off\"):  # Adjust the file extension as needed\n",
    "                model_files.append(os.path.join(root, file))\n",
    "    return model_files\n",
    "\n",
    "def load_model_data(file_path):\n",
    "    # Implement your code to load 3D model data from the file\n",
    "    # You might use the readOff function or any other method\n",
    "    # Return the loaded model data\n",
    "    pass\n",
    "\n",
    "def generate_dataset(data_folder):\n",
    "    X_train = []\n",
    "    X_test = []\n",
    "\n",
    "    # Iterate over the object class folders\n",
    "    for class_folder in tqdm(os.listdir(data_folder)):\n",
    "        if class_folder.lower() == 'bathtub':  # Skip the \"bathtub\" folder because of bad .off format\n",
    "            continue\n",
    "\n",
    "        class_path = os.path.join(data_folder, class_folder)\n",
    "\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        train_folder = os.path.join(class_path, 'train')\n",
    "        test_folder = os.path.join(class_path, 'test')\n",
    "\n",
    "        # Load train set\n",
    "        for file in os.listdir(train_folder):\n",
    "            model_file = os.path.join(train_folder, file)\n",
    "            model_data = load_model_data(model_file)\n",
    "            X_train.append(model_data)\n",
    "\n",
    "        # Load test set\n",
    "        for file in os.listdir(test_folder):\n",
    "            model_file = os.path.join(test_folder, file)\n",
    "            model_data = load_model_data(model_file)\n",
    "            X_test.append(model_data)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "\n",
    "    return X_train, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Mesh' object has no attribute 'voxelize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theca\\Documents\\Python\\ML\\main.ipynb Cell 5\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m voxels\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m# np.save(output_path, voxels)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m voxels \u001b[39m=\u001b[39m off_to_voxels(\u001b[39m'\u001b[39;49m\u001b[39m./ModelNet40/chair/train/chair_0001.off\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# voxel_array = voxelize('./ModelNet40/chair/train/chair_0001.off')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# print(voxel_array)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\theca\\Documents\\Python\\ML\\main.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m voxel_dim \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m voxel_bounds \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m voxels \u001b[39m=\u001b[39m stl_mesh\u001b[39m.\u001b[39;49mvoxelize(dim\u001b[39m=\u001b[39m[voxel_dim, voxel_dim, voxel_dim], bounds\u001b[39m=\u001b[39mvoxel_bounds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# Save or return voxels as needed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X15sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mreturn\u001b[39;00m voxels\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Mesh' object has no attribute 'voxelize'"
     ]
    }
   ],
   "source": [
    "from stl import mesh\n",
    "import tripy\n",
    "def read_off(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    num_vertices, num_faces, _ = map(int, lines[1].split())\n",
    "\n",
    "    vertices = []\n",
    "    for line in lines[2:2 + num_vertices]:\n",
    "        x, y, z = map(float, line.split())\n",
    "        vertices.append([x, y, z])\n",
    "\n",
    "    faces = []\n",
    "    for line in lines[2 + num_vertices:]:\n",
    "        face = list(map(int, line.split()[1:]))\n",
    "        faces.append(face)\n",
    "\n",
    "    return np.array(vertices), np.array(faces)\n",
    "\n",
    "def off_to_voxels(input_path, voxel_dim=32, voxel_bounds=(0, 1, 0, 1, 0, 1)):\n",
    "    with open(input_path, 'r') as off_file:\n",
    "        lines = off_file.readlines()\n",
    "\n",
    "    vertex_count, face_count, _ = map(int, lines[1].split())\n",
    "\n",
    "    vertices = []\n",
    "    for line in lines[2:2 + vertex_count]:\n",
    "        vertices.append(list(map(float, line.split()[0:3])))\n",
    "\n",
    "    faces = []\n",
    "    for line in lines[2 + vertex_count:]:\n",
    "        faces.append(list(map(int, line.split()[1:])))\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "    stl_mesh = mesh.Mesh(np.zeros(len(faces), dtype=mesh.Mesh.dtype))\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        for j in range(3):\n",
    "            stl_mesh.vectors[i][j] = vertices[face[j]]\n",
    "\n",
    "    vedo_stl_mesh = vedo_mesh(stl_mesh.vectors)\n",
    "    voxels = vedo_stl_mesh.voxelize(dim=[voxel_dim, voxel_dim, voxel_dim], bounds=voxel_bounds)\n",
    "\n",
    "    return voxels\n",
    "def off_to_voxels(input_path):\n",
    "    with open(input_path, 'r') as off_file:\n",
    "        lines = off_file.readlines()\n",
    "\n",
    "    vertex_count, face_count, _ = map(int, lines[1].split())\n",
    "\n",
    "    vertices = []\n",
    "    for line in lines[2:2 + vertex_count]:\n",
    "        vertices.append(list(map(float, line.split()[0:3])))\n",
    "\n",
    "    faces = []\n",
    "    for line in lines[2 + vertex_count:]:\n",
    "        faces.append(list(map(int, line.split()[1:])))\n",
    "\n",
    "    vertices = np.array(vertices)\n",
    "    stl_mesh = mesh.Mesh(np.zeros(len(faces), dtype=mesh.Mesh.dtype))\n",
    "\n",
    "    for i, face in enumerate(faces):\n",
    "        for j in range(3):\n",
    "            stl_mesh.vectors[i][j] = vertices[face[j]]\n",
    "\n",
    "    # Voxelization\n",
    "    voxel_dim = 32\n",
    "    voxel_bounds = (0, 1, 0, 1, 0, 1)\n",
    "    voxels = stl_mesh.voxelize(dim=[voxel_dim, voxel_dim, voxel_dim], bounds=voxel_bounds)\n",
    "\n",
    "    # Save or return voxels as needed\n",
    "    return voxels\n",
    "    # np.save(output_path, voxels)\n",
    "voxels = off_to_voxels('./ModelNet40/chair/train/chair_0001.off')\n",
    "\n",
    "# voxel_array = voxelize('./ModelNet40/chair/train/chair_0001.off')\n",
    "# print(voxel_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readOff(filename):\n",
    "    f = open(filename)\n",
    "    f.readline()\n",
    "    nvertices, nfaces, nedges = map(int, f.readline().split())\n",
    "    vertices = []\n",
    "    for _ in range(nvertices):\n",
    "        vertices.append(list(map(float, f.readline().strip().split())))\n",
    "    vertices = np.array(vertices)\n",
    "\n",
    "    triangles = []\n",
    "    for _ in range(nfaces):\n",
    "        face = list(map(int, f.readline().strip().split()))\n",
    "        ntriangles, verts = face[0] - 3 + 1, face[1:]\n",
    "        for n in range(ntriangles):\n",
    "            triangles.append([verts[0], verts[1 + n], verts[2 + n]])\n",
    "    triangles = np.array(triangles)\n",
    "\n",
    "    return vertices, triangles\n",
    "\n",
    "def save_png(off_filename, png_filename):\n",
    "    vertices, faces = readOff(off_filename)\n",
    "\n",
    "    x, y, z = vertices.T\n",
    "    I, J, K = faces.T\n",
    "\n",
    "    mesh = go.Mesh3d(\n",
    "        x=-x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        i=I,\n",
    "        j=J,\n",
    "        k=K,\n",
    "        name='',\n",
    "        showscale=False,\n",
    "        color='brown'\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=mesh)\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False)\n",
    "        )\n",
    "    )\n",
    "    fig.write_image(png_filename)\n",
    "    # print(f\"Image saved as {png_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_data(data_folder, output_folder):\n",
    "    for class_folder in os.listdir(data_folder):\n",
    "        if class_folder.lower() == 'bathtub':  # Skip the \"bathtub\" folder\n",
    "            continue\n",
    "\n",
    "        class_path = os.path.join(data_folder, class_folder)\n",
    "\n",
    "        # Skip non-directory items\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        output_class_folder = os.path.join(output_folder, class_folder)\n",
    "        os.makedirs(output_class_folder, exist_ok=True)\n",
    "\n",
    "        for split_folder in ['train', 'test']:\n",
    "            split_path = os.path.join(class_path, split_folder)\n",
    "\n",
    "            for model_file in os.listdir(split_path):\n",
    "                if model_file.endswith(\".off\"):  # Adjust the file extension as needed\n",
    "                    off_file_path = os.path.join(split_path, model_file)\n",
    "                    png_file_name = f\"{model_file[:-4]}_{split_folder}.png\"\n",
    "                    png_file_path = os.path.join(output_class_folder, png_file_name)\n",
    "\n",
    "                    save_png(off_file_path, png_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = generate_dataset(dataset_path)\n",
    "#generate_and_save_data(dataset_path, images_path)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Voxel Shape: (None, 32, 32, 32, 1)\n",
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_89 (InputLayer)       [(None, 128, 128, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " input_90 (InputLayer)       [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " model_30 (Functional)       (None, 32, 32, 32, 1)        2697073   ['input_89[0][0]',            \n",
      "                                                          29         'input_90[0][0]']            \n",
      "                                                                                                  \n",
      " model_29 (Functional)       (None, 1)                    7617760   ['model_30[0][0]',            \n",
      "                                                                     'input_90[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 277325089 (1.03 GB)\n",
      "Trainable params: 269703777 (1.00 GB)\n",
      "Non-trainable params: 7621312 (29.07 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dict_size = 1\n",
    "\n",
    "discriminator = make_discriminator(dict_size, 'voxels-use')\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "\n",
    "discriminator.trainable = False\n",
    "\n",
    "generator = _generator_v_3d(dict_size)\n",
    "gan_input = [Input(shape=(128, 128, 1)), Input(shape=(1,))]\n",
    "generated_voxel = generator(gan_input)\n",
    "print(\"Generated Voxel Shape:\", generated_voxel.shape)\n",
    "\n",
    "validity = discriminator([generated_voxel, gan_input[1]])\n",
    "gan = Model(gan_input, validity)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theca\\Documents\\Python\\ML\\main.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Generate a batch of new images\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, dict_size, batch_size)\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m generated_images \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39;49mpredict([real_images, labels])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Train the discriminator\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m d_loss_real \u001b[39m=\u001b[39m discriminator\u001b[39m.\u001b[39mtrain_on_batch([real_images, labels], np\u001b[39m.\u001b[39mones((batch_size, \u001b[39m1\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\theca\\Documents\\Python\\ML\\env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\theca\\Documents\\Python\\ML\\env\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "epochs = 10000  # Adjust as needed\n",
    "batch_size = 64  # Adjust as needed\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_images = X_train[idx]\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    labels = np.random.randint(0, dict_size, batch_size).reshape((-1, 1))\n",
    "    generated_images = generator.predict([real_images, labels])\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([real_images, labels], np.ones((batch_size, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch([generated_images, labels], np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    labels = np.random.randint(0, dict_size, batch_size).reshape((-1, 1))\n",
    "    valid_y = np.array([1] * batch_size).reshape((-1, 1))\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = gan.train_on_batch([real_images, labels], valid_y)\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100 * d_loss[1]}] [G loss: {g_loss}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theca\\Documents\\Python\\ML\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, dict_size, batch_size)\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m generated_images \u001b[39m=\u001b[39m generator\u001b[39m.\u001b[39mpredict([X_test, test_labels])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m generator\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mgenerator_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theca/Documents/Python/ML/main.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m discriminator\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mdiscriminator_model.h5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'generator' is not defined"
     ]
    }
   ],
   "source": [
    "test_labels = np.random.randint(0, dict_size, batch_size).reshape((-1, 1))\n",
    "generated_images = generator.predict([X_test, test_labels])\n",
    "\n",
    "generator.save('generator_model.h5')\n",
    "discriminator.save('discriminator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
